Recurrent Neural Nerworks:
with hidden statt
e/g/
y_(t-1)
|
h_(t-1) -> h(t)...
|
x_(t-1)

h_t depends on h_(t-1), x_t, y_t depends on h_t

h_t carries longer-range contex
RNN have vanishing grad provlem 

LSTM Long Short-Term Memory Networks
 add forget gate, input gate, cell state, output gates
 tf.contrib.rnn.LSTMCell
