Bootstrap(or resampling):  sampling from the empirical distribution   (good for reduce veriance)
          application: use resampling to generate weak learners for classification
          
Bagging:  Bootstrap aggregation
          Train uncorrelated weak learners on resampled training sets
          
representation:
            x_i in class k       y = (0,...,1,0 ...)' 1 at kth entry

Algotirhm:
bootstrap sample B_b of size n then train classifier on B_b denote as fb

Classification: f = 1/b sum f_b = (p1,...pk)' classify as marmax

e.g. bagging with trees

Note:
bagging good with trees, since trees usually have high var
boosting typically outperforms bagging, since boosting better at reducting correlation
            
Random Forests: modefication of bagging
want to furthre reduce correlation
choose a different subset of dimensions at each split

training:
Draw a bootstrap sample B_b of size n from training data

train f_b on B_b. where in each split:
    Select m axes in Rd at random and spli allowed only on this m axis subset
 
 
Classification: f = 1/b sum f_b = (p1,...pk)' classify as marmax

Recommendation m<= SQRT(d)
